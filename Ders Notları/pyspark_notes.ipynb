{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5c6ec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "spark = SparkSession.builder.master(\"local[4]\").appName(\"giveatry\").getOrCreate()\n",
    "df = spark.read.format('csv').options(delimiter=',',header=\"True\",inferschema=\"True\").load(\"C:\\\\Users\\\\brhng\\\\Desktop\\\\Advertising.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6954c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Bender Part\n",
    "df.describe().show() #->describe\n",
    "df.columns() #->columns names list\n",
    "df.select([\"column1\",\"column2\"]).show() #-> columns selection\n",
    "df.printSchema() #-> df.info()\n",
    "df.dtypes #-> columns types\n",
    "df.withColumn(\"column3\",df[\"column2\"]+125) #-> inplace=false \n",
    "df.drop(\"column1\")#-> inplace=false\n",
    "df.withColumnRenamed(\"Name\",\"New Name\") #-> column renamed\n",
    "df.na.drop() #-> dropping nan values (row)\n",
    "    #how == any(default at least 1) or all(all of them should be nan for dropping) \n",
    "    #threshold = 2(at leats 2 non value)\n",
    "    #subset[\"column_name\"] (just column_name daki nanlar silinir)\n",
    "df.na.fill(\"0\",[\"column1\",\"column2\"]) #-> 0 ile dolduruyor\n",
    "df.filter(\"column1 <= 1000\").select([\"column1\",\"column2\"]).show()\n",
    "df.filter(~(df[\"column1\"]<2000)&(df[\"column1\"]!=0)).show()\n",
    "df.groupBy(\"columne1\"). sum(),mean(),count(),max(),...\n",
    "df.agg({\"column1\":\"sum\"}).show()\n",
    "df.dropDuplicates([\"department\", \"salary\"])  # ikisnin de aynı olanlar duplicate sayılıcak\n",
    "distinct_amount = str(df.distinct()) #unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc85f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot sabit demek basketballdaki gibi bir sutünü sabit tutup diyerlerini\n",
    "pivotDf = df.groupby(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDf.printSchema()\n",
    "pivotDf.show(truncate = False)\n",
    "#her productın ülkeler özelindeki satış amountları tablosu\n",
    "\n",
    "# +-------+------+-----+------+----+\n",
    "# |Product|Canada|China|Mexico|USA |\n",
    "# +-------+------+-----+------+----+\n",
    "# |Orange |null  |4000 |null  |4000|\n",
    "# |Beans  |null  |1500 |2000  |1600|\n",
    "# |Banana |2000  |4000 |null  |1000|\n",
    "# |Carrots|2000  |1200 |null  |1500|\n",
    "# +-------+------+-----+------+----+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f087e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word count example\n",
    "veri_seti =\"C:\\\\Users\\\\brhng\\\\Desktop\\\\ibbdata-lojistic.txt\"\n",
    "istac_rdd =sc.textFile(veri_seti)\n",
    "istac_rdd.count() #satır sayısı\n",
    "istac_rdd.take(5) #ilk 5 satır\n",
    "istac_rdd_kelimeler = istac_rdd.flatMap(lambda satir: satir.split(\" \")) #satırlardan kelimelere\n",
    "istac_rdd_kelimeler_sayilari = istac_rdd_kelimeler.map(lambda kelime:(kelime,1)) #wordcount\n",
    "istac_rdd_kelimeler_sayilar_reduce = istac_rdd_kelimeler_sayilari.reduceByKey(lambda x,y:(x+y))\n",
    "istac_rdd_kelimeler_sayilar_reduce_sort = istac_rdd_kelimeler_sayilar_reduce.map(lambda x:(x[1],x[0]))\n",
    "istac_rdd_kelimeler_sayilari_reduce_sort.sortByKey(False).take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fca6abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creatae DataFrame\n",
    "data = [(\"James\", \"\", \"William\", \"36636\", \"M\", 3000), (\"Michael\", \"Smith\", \"\", \"40288\", \"M\", 4000), (\"Robert\", \"\", \"Dawson\", \"42114\", \"M\", 4000), \n",
    "        (\"Maria\", \"\", \"Jones\", \"39192\", \"F\", 4000)]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"firstname\", StringType(), True),\\\n",
    "    StructField(\"middlename\", StringType(), True),\\\n",
    "    StructField(\"lastname\", StringType(), True),\\\n",
    "    StructField(\"id\", StringType(), True),\\\n",
    "    StructField(\"gender\", StringType(), True),\\\n",
    "    StructField(\"salary\", IntegerType(), True)\\\n",
    "    ])\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d7c132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML with pyspark\n",
    "#-> featurlaresimi tek bir feature olarak birleştiriyoruz\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "feature_assembler = VectorAssembler(inputCols=[\"TV\",\"Radio\"],outputCol=\"new_column_name\")\n",
    "new_fet = feature_assembler.transform(df)\n",
    "finalized_data = new_fet.select([\"new_columns_name\",\"label\"])\n",
    "\n",
    "#-> LinearRegresison\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "train_data,test_data =finalized_data.randomSplit([0.75,0.25])\n",
    "regressor = LinearRegression(featuresCol=\"new_columns_name\",labelCol=\"label\")\n",
    "regressor = regressor.fit(train_data)\n",
    "regressor.coefficients#a\n",
    "regreesor.intercept #b\n",
    "pred_res = regressor.evalutate(test_data)\n",
    "pred.res.prediction.show()\n",
    "\n",
    "#pipeline\n",
    "\n",
    "lr = LinearRegression()\n",
    "pipeline = Pipeline(stages = [featureIndexer, lr])\n",
    "model = pipeline.fit(trainingData)\n",
    "predictions = model.transform(testData)\n",
    "predictions.select(\"features\", \"label\", \"prediction\").show(5)\n",
    "evaluator = RegressionEvaluator(labelCol = 'label', predictionCol = 'prediction', metricName = 'rmse')\n",
    "rmse = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5796984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "quality_udf = udf(lambda x : condition(x), StringType()) #condition bir fonksiyon , istenilien çıktı türü \n",
    "#pd.cut veya progress_apply yerin kullanılabilir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6885a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer, StringIndexer, IndexToString\n",
    "labelIndexer = StringIndexer(inputCol = 'label', outputCol = 'indexedLabel').fit(transformed)\n",
    "labelIndexer.transform(transformed).show(5, True)\n",
    "\n",
    "featureIndexer = VectorIndexer(inputCol = 'features', outputCol = 'indexedFeatures', maxCategories = 4).fit(transformed)\n",
    "featureIndexer.transform(transformed).show(5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
